{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd349d97",
   "metadata": {},
   "source": [
    "Welcome to Fugue SQL demo\n",
    "\n",
    "In this demo, we are going to explore US Flight Dataset that is extensively discussed on the internet. \n",
    "\n",
    "I will show you how to use Fugue and Spark to quickly iterate on big data problems.\n",
    "\n",
    "\n",
    "Firstly, I will paste a piece of code to initialize the notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-graphic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:01:35.805781Z",
     "start_time": "2021-07-16T06:01:30.266493Z"
    }
   },
   "outputs": [],
   "source": [
    "from fugue_notebook import setup\n",
    "import fugue_spark\n",
    "\n",
    "setup()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "(SparkSession.builder\n",
    "         .config(\"spark.driver.memory\", \"30g\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "         .config(\"fugue.spark.use_pandas_udf\", True)\n",
    "         .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79755ae5",
   "metadata": {},
   "source": [
    "The setup function will create shortcut to use spark \n",
    "\n",
    "It also enables the Fugue SQL magic and syntax highlighting\n",
    "\n",
    "Please pay attention to the config for Spark session.\n",
    "\n",
    "We enabled fugue to use pandas udf whenever is eligible.\n",
    "\n",
    "In fugue, using or not using pandas udf, your code will not need any change\n",
    "\n",
    "It's just a switch.\n",
    "\n",
    "And for this case, enabling pandas udf makes certain steps 3-4 times faster.\n",
    "\n",
    "Also notice you don't need to write this piece of code every time you use Fugue.\n",
    "\n",
    "There can be a centralized place to customize your fugue experience.\n",
    "\n",
    "-\n",
    "\n",
    "All data of this demo has been saved on google cloud\n",
    "\n",
    "The first fugue SQL is to load csv files from google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-warehouse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:01:50.283677Z",
     "start_time": "2021-07-16T06:01:48.955467Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql\n",
    "airports = \n",
    "    LOAD CSV \"gs://fugue/demo/flights/airports.csv\"\n",
    "    COLUMNS airport_id:long,name:str,city:str,country:str,iata:str,icao:str,lat:double,lng:double,alt:long,timezone:str,dst:str,type:str,source:str\n",
    "YIELD DATAFRAME\n",
    "PRINT\n",
    "\n",
    "airlines = \n",
    "    LOAD CSV \"gs://fugue/demo/flights/airlines.csv\"\n",
    "    COLUMNS airline_id:long,name:str,alias:str,iata:str,icao:str,callsign:str,country:str,active:str\n",
    "YIELD DATAFRAME\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f1d4a",
   "metadata": {},
   "source": [
    "These csv files don't have header, so we define the schema using COLUMNS\n",
    "\n",
    "Then we yield them so we can consume them in the following cells\n",
    "\n",
    "We also print them for debugging purposes\n",
    "\n",
    "Notice the execution of this cell has nothing to do with Spark.\n",
    "\n",
    "It is using default exeuction engine, which only uses native python and pandas\n",
    "\n",
    "-\n",
    "\n",
    "now let's take a look at the underlying dataframe of the yielded airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-attempt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:01:51.417860Z",
     "start_time": "2021-07-16T06:01:51.386372Z"
    }
   },
   "outputs": [],
   "source": [
    "airports.native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775f949",
   "metadata": {},
   "source": [
    "it is just a pandas dataframe.\n",
    "\n",
    "-\n",
    "\n",
    "Now let's explore the main dataset, the flights information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "LOAD \"gs://fugue/demo/flights/flights.parquet\"\n",
    "PRINT ROWCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73b6c2",
   "metadata": {},
   "source": [
    "The flights.parquet is a folder of parquet files, using Spark will be much faster\n",
    "\n",
    "to use spark, we just specify spark for fsql\n",
    "\n",
    "it has 2.5 million rows\n",
    "\n",
    "-\n",
    "-\n",
    "\n",
    "\n",
    "Now we need to do some transformation on this big dataset\n",
    "\n",
    "But how can we verify everything is working fine before applying it on the entire dataset?\n",
    "\n",
    "We can use the Fugue SQL's SAMPLE syntax here and yield a small dataframe to iterate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-tiffany",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:06:54.460521Z",
     "start_time": "2021-07-16T06:06:51.408082Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "LOAD \"gs://fugue/demo/flights/flights.parquet\"\n",
    "PRINT ROWCOUNT\n",
    "SAMPLE 0.01 PERCENT SEED 0 PERSIST\n",
    "YIELD DATAFRAME AS test\n",
    "PRINT ROWCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4ec6a",
   "metadata": {},
   "source": [
    "In test, we have only 267 rows\n",
    "\n",
    "-\n",
    "\n",
    "If you want to iterate quickly or do unit test,using small dataset is one thing\n",
    "\n",
    "Avoid using Spark is also a great idea although practically at this point, if you were not using Fugue, you already see Spark dependencies everywhere in your code, bypassing Spark would not be an option any more.\n",
    "\n",
    "But so far with Fugue, you don't see Spark dependencies anywhere.\n",
    "\n",
    "-\n",
    "\n",
    "Let's paste the transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-transmission",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T07:51:47.495241Z",
     "start_time": "2021-07-16T07:51:47.395383Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#schema: *,ts:datetime,day_of_year:int,hour_of_week:int\n",
    "def generate_time_metrics(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    date = df[\"FL_DATE\"].astype(str) + \" \"+df[\"CRS_DEP_TIME\"].astype(str)\n",
    "    df[\"ts\"]=pd.to_datetime(date, format=\"%Y-%m-%d %H%M\")\n",
    "    df[\"day_of_year\"]=df[\"ts\"].dt.dayofyear\n",
    "    df[\"hour_of_week\"]=df[\"ts\"].dt.dayofweek*24+df[\"ts\"].dt.hour\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941c3d7",
   "metadata": {},
   "source": [
    "Again, this function is still unrelated to Spark, or Fugue.\n",
    "\n",
    "It is just a native python and pandas function, adding a few more columns from the existing columns\n",
    "\n",
    "The comment line will be useful to fugue\n",
    "\n",
    "it tells fugue the output schema is the input schema which is the wildcard plus three columns with types\n",
    "\n",
    "But since it's just a comment line, it doesn't build any dependency on Fugue\n",
    "\n",
    "We all know how to unit test such a simple function on pandas\n",
    "\n",
    "And here, we just verify the result by the sample test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_time_metrics(test.as_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2eb5",
   "metadata": {},
   "source": [
    "OK, this code only works for pandas dataframes.\n",
    "\n",
    "But with fugue this function can work for all supported dataframes such Spark and dask\n",
    "\n",
    "Let's see how to use this function in Fugue SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "TRANSFORM test USING generate_time_metrics\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2be94",
   "metadata": {},
   "source": [
    "TRANSFORM can directly apply simple python functions on a generalized dataframe\n",
    "\n",
    "The usage has many variations, for example, you can pre partition the dataframe then the function will be applied on each partition separately.\n",
    "\n",
    "For more details, you can visit the Fugue repository\n",
    "\n",
    "-\n",
    "\n",
    "Now, we want to do one more thing, the output dataframe has too many columns\n",
    "\n",
    "we want to select just a few and rename them\n",
    "\n",
    "This is a typical case that SQL SELECT can do an elegant job, so let's just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-blues",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:07:11.149804Z",
     "start_time": "2021-07-16T06:07:10.301541Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql\n",
    "TRANSFORM test USING generate_time_metrics\n",
    "SELECT \n",
    "    ts, \n",
    "    day_of_year, \n",
    "    hour_of_week, \n",
    "    ORIGIN AS origin,\n",
    "    DEST AS dest,\n",
    "    OP_UNIQUE_CARRIER AS carrier,\n",
    "    DEP_DELAY AS delay\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53929cd",
   "metadata": {},
   "source": [
    "This still runs on pandas, not Spark\n",
    "\n",
    "In Fugue, SQL is no longer a privilege when using a certain computing framework.\n",
    "\n",
    "Your SQL statements can run on every computing framework Fugue supports, including native pandas engine.\n",
    "\n",
    "-\n",
    "\n",
    "So this piece of code has a map function plus a sql statement\n",
    "\n",
    "You could make it more complicated, it just runs without Spark.\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "Now let's bring it to Spark and apply on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-fountain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:08:30.167293Z",
     "start_time": "2021-07-16T06:07:54.061673Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "LOAD \"gs://fugue/demo/flights/flights.parquet\"\n",
    "TRANSFORM USING generate_time_metrics\n",
    "SELECT \n",
    "    ts, \n",
    "    day_of_year, \n",
    "    hour_of_week, \n",
    "    ORIGIN AS origin,\n",
    "    DEST AS dest,\n",
    "    OP_UNIQUE_CARRIER AS carrier,\n",
    "    DEP_DELAY AS delay\n",
    "PERSIST \n",
    "YIELD DATAFRAME AS flights\n",
    "PRINT ROWCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e100a",
   "metadata": {},
   "source": [
    "We just replace the test dataframe with a LOAD statement, and specify the engine spark for fsql\n",
    "\n",
    "We also yield the result to use later\n",
    "\n",
    "While it is running, let's continue our demo\n",
    "\n",
    "-\n",
    "\n",
    "Visualization is very important for data exploration\n",
    "\n",
    "so how can we enable visualization inside Fugue SQL?\n",
    "\n",
    "Again, you only need to write the very simple python functions independent from Fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marine-pressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T18:58:42.410154Z",
     "start_time": "2021-07-16T18:58:42.395693Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df:pd.DataFrame,x,y,sort,**kwargs) -> None:\n",
    "    df.sort_values(sort).plot(x=x,y=y,**kwargs)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_bar(df:pd.DataFrame,x,y,sort,**kwargs) -> None:\n",
    "    df.sort_values(sort).plot.bar(x=x,y=y,**kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314feae7",
   "metadata": {},
   "source": [
    "plot and plot_bar will be used as another type of extension in Fugue, it's called outputter, it has to run on the driver.\n",
    "\n",
    "Here is how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-ownership",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:09:16.356303Z",
     "start_time": "2021-07-16T06:09:14.131586Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "SELECT day_of_year, AVG(delay) AS avg_delay FROM flights GROUP BY day_of_year\n",
    "OUTPUT USING plot(x=\"day_of_year\",y=\"avg_delay\",sort=\"day_of_year\")\n",
    "\n",
    "SELECT hour_of_week, AVG(delay) AS avg_delay FROM flights GROUP BY hour_of_week\n",
    "OUTPUT USING plot(x=\"hour_of_week\",y=\"avg_delay\",sort=\"hour_of_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccbfce",
   "metadata": {},
   "source": [
    "This is a simple piece of code shows several things\n",
    "\n",
    "First, it is multi task, we aggregated and ploted data points based on day of year and hour of year respectively\n",
    "\n",
    "Second, it is a more balanced case where you use both standard SELECT statements and Fugue specific syntax OUTPUT\n",
    "\n",
    "Third, and again, this code runs on all computing frameworks Fugue supports\n",
    "\n",
    "-\n",
    "\n",
    "Now let's see a more SQL intensive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-score",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:10:06.438435Z",
     "start_time": "2021-07-16T06:09:46.396557Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "info = \n",
    "    SELECT ts\n",
    "        , carrier\n",
    "        , B.name AS carrier_name\n",
    "        , origin\n",
    "        , C.name AS origin_name      \n",
    "        , C.country AS origin_country      \n",
    "        , C.lat AS origin_lat       \n",
    "        , C.lng AS origin_lng    \n",
    "        , dest\n",
    "        , D.name AS dest_name\n",
    "        , D.country AS dest_country    \n",
    "        , D.lat AS dest_lat       \n",
    "        , D.lng AS dest_lng    \n",
    "        , delay\n",
    "    FROM flights AS A\n",
    "    LEFT OUTER JOIN airlines AS B\n",
    "        ON A.carrier = B.iata\n",
    "    LEFT OUTER JOIN airports AS C\n",
    "        ON A.origin = C.iata\n",
    "    LEFT OUTER JOIN airports AS D\n",
    "        ON A.dest = D.iata\n",
    "    WHERE C.lat IS NOT NULL AND C.lng IS NOT NULL\n",
    "        AND D.lat IS NOT NULL AND D.lng IS NOT NULL\n",
    "PERSIST YIELD DATAFRAME\n",
    "PRINT ROWCOUNT\n",
    "\n",
    "SELECT * WHERE origin_country = dest_country AND origin_country = 'United States'\n",
    "PERSIST YIELD DATAFRAME AS info_us\n",
    "PRINT ROWCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470b117",
   "metadata": {},
   "source": [
    "We joined flights, airlines and airports together as info\n",
    "\n",
    "Then we filtered the dataframe to contain only US domestic flights.\n",
    "\n",
    "We yielded both and print both\n",
    "\n",
    "-\n",
    "\n",
    "Now let's do more analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-upper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:15:17.139800Z",
     "start_time": "2021-07-16T06:15:10.789722Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "SELECT origin, AVG(delay) AS delay FROM info_us GROUP BY origin\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"origin\",y=\"delay\",sort=\"delay\", title=\"By Origin\")\n",
    "\n",
    "SELECT dest, AVG(delay) AS delay FROM info_us GROUP BY dest\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"dest\",y=\"delay\",sort=\"delay\", title=\"By Dest\")\n",
    "\n",
    "top = \n",
    "    SELECT carrier, COUNT(*) AS ct \n",
    "    FROM info_us GROUP BY carrier \n",
    "    ORDER BY ct DESC LIMIT 10\n",
    "    \n",
    "SELECT info_us.* FROM info_us INNER JOIN top ON info_us.carrier = top.carrier\n",
    "SELECT carrier_name, AVG(delay) AS delay GROUP BY carrier_name\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"carrier_name\",y=\"delay\",sort=\"delay\", title=\"By Top Carriers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac5563",
   "metadata": {},
   "source": [
    "in this example, pay attention to that we can use select statements one after another\n",
    "\n",
    "this avoids gigantic embeded SQL statements\n",
    "\n",
    "actually fugue also supports embeded SQL, if you prefer, you can still write in that way\n",
    "\n",
    "also see how we use simple assignment syntax to define intermediate dataframes\n",
    "\n",
    "and how we can use them in the following steps\n",
    "\n",
    "-\n",
    "\n",
    "another thing worth to mention is anonimity\n",
    "\n",
    "we can give names to dataframes only when necessary\n",
    "\n",
    "for chaining operations we can keep them anonymous, and we also omitted a lot of FROM clauses\n",
    "\n",
    "there is no ambiguity issue.\n",
    "\n",
    "And of course you can be explicit on each step in Fugue,\n",
    "\n",
    "you can assign variable names and keep FROM clause if you prefer.\n",
    "\n",
    "-\n",
    "\n",
    "Now imagine how to implement the same logic in Spark,\n",
    "\n",
    "you may realize how much bolierplate code to write to achieve the same thing\n",
    "\n",
    "Fugue revolutionized SQL, the logic is much simpler, cleaner and more intuitive\n",
    "\n",
    "-\n",
    "\n",
    "Now let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-centre",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:18:30.084718Z",
     "start_time": "2021-07-16T06:17:45.464417Z"
    }
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "LOAD \"gs://fugue/demo/flights/flights.parquet\"\n",
    "TRANSFORM USING generate_time_metrics\n",
    "flights = \n",
    "    SELECT \n",
    "        ts, \n",
    "        day_of_year, \n",
    "        hour_of_week, \n",
    "        ORIGIN AS origin,\n",
    "        DEST AS dest,\n",
    "        OP_UNIQUE_CARRIER AS carrier,\n",
    "        DEP_DELAY AS delay\n",
    "    PERSIST \n",
    "    \n",
    "SELECT day_of_year, AVG(delay) AS avg_delay FROM flights GROUP BY day_of_year\n",
    "OUTPUT USING plot(x=\"day_of_year\",y=\"avg_delay\",sort=\"day_of_year\")\n",
    "\n",
    "SELECT hour_of_week, AVG(delay) AS avg_delay FROM flights GROUP BY hour_of_week\n",
    "OUTPUT USING plot(x=\"hour_of_week\",y=\"avg_delay\",sort=\"hour_of_week\")\n",
    "\n",
    "    \n",
    "info = \n",
    "    SELECT ts\n",
    "        , carrier\n",
    "        , B.name AS carrier_name\n",
    "        , origin\n",
    "        , C.name AS origin_name      \n",
    "        , C.country AS origin_country      \n",
    "        , C.lat AS origin_lat       \n",
    "        , C.lng AS origin_lng    \n",
    "        , dest\n",
    "        , D.name AS dest_name\n",
    "        , D.country AS dest_country    \n",
    "        , D.lat AS dest_lat       \n",
    "        , D.lng AS dest_lng    \n",
    "        , delay\n",
    "    FROM flights AS A\n",
    "    LEFT OUTER JOIN airlines AS B\n",
    "        ON A.carrier = B.iata\n",
    "    LEFT OUTER JOIN airports AS C\n",
    "        ON A.origin = C.iata\n",
    "    LEFT OUTER JOIN airports AS D\n",
    "        ON A.dest = D.iata\n",
    "    WHERE C.lat IS NOT NULL AND C.lng IS NOT NULL\n",
    "        AND D.lat IS NOT NULL AND D.lng IS NOT NULL\n",
    "\n",
    "info_us = \n",
    "    SELECT * WHERE origin_country = dest_country AND origin_country = 'United States'\n",
    "    PERSIST\n",
    "    \n",
    "SELECT origin, AVG(delay) AS delay FROM info_us GROUP BY origin\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"origin\",y=\"delay\",sort=\"delay\", title=\"By Origin\")\n",
    "\n",
    "SELECT dest, AVG(delay) AS delay FROM info_us GROUP BY dest\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"dest\",y=\"delay\",sort=\"delay\", title=\"By Dest\")\n",
    "\n",
    "top = \n",
    "    SELECT carrier, COUNT(*) AS ct \n",
    "    FROM info_us GROUP BY carrier \n",
    "    ORDER BY ct DESC LIMIT 20\n",
    "    \n",
    "SELECT info_us.* FROM info_us INNER JOIN top ON info_us.carrier = top.carrier\n",
    "SELECT carrier_name, AVG(delay) AS delay GROUP BY carrier_name\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"carrier_name\",y=\"delay\",sort=\"delay\", title=\"By Top Carriers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb66ab",
   "metadata": {},
   "source": [
    "We just copy pasted the code together and removed the YIELD statements because they are in a single cell now, YIELD is not necessary\n",
    "\n",
    "So you can see Fugue SQL is a first class programming language to describe complex data pipelines\n",
    "\n",
    "It's multi task, it's great for SQL heavy pipelines where python can help at a few steps\n",
    "\n",
    "while it's running, let's talk about productionization\n",
    "\n",
    "-\n",
    "\n",
    "notebook magic is a cool idea, it makes development experience much better\n",
    "\n",
    "however, for production, we need modulized, testable code\n",
    "\n",
    "Let's see how we modulize the previous logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-cleveland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:19:06.916534Z",
     "start_time": "2021-07-16T06:19:06.910951Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_query = \"\"\"\n",
    "airports = \n",
    "    LOAD CSV \"gs://fugue/demo/flights/airports.csv\"\n",
    "    COLUMNS airport_id:long,name:str,city:str,country:str,iata:str,icao:str,lat:double,lng:double,alt:long,timezone:str,dst:str,type:str,source:str\n",
    "YIELD DATAFRAME\n",
    "\n",
    "airlines = \n",
    "    LOAD CSV \"gs://fugue/demo/flights/airlines.csv\"\n",
    "    COLUMNS airline_id:long,name:str,alias:str,iata:str,icao:str,callsign:str,country:str,active:str\n",
    "YIELD DATAFRAME\n",
    "\"\"\"\n",
    "\n",
    "flights_query = \"\"\"\n",
    "LOAD \"gs://fugue/demo/flights/flights.parquet\"\n",
    "TRANSFORM USING generate_time_metrics\n",
    "flights = \n",
    "    SELECT \n",
    "        ts, \n",
    "        day_of_year, \n",
    "        hour_of_week, \n",
    "        ORIGIN AS origin,\n",
    "        DEST AS dest,\n",
    "        OP_UNIQUE_CARRIER AS carrier,\n",
    "        DEP_DELAY AS delay\n",
    "    PERSIST YIELD DATAFRAME\n",
    "\"\"\"\n",
    "\n",
    "eda_query = \"\"\"\n",
    "SELECT day_of_year, AVG(delay) AS avg_delay FROM flights GROUP BY day_of_year\n",
    "OUTPUT USING plot(x=\"day_of_year\",y=\"avg_delay\",sort=\"day_of_year\")\n",
    "\n",
    "SELECT hour_of_week, AVG(delay) AS avg_delay FROM flights GROUP BY hour_of_week\n",
    "OUTPUT USING plot(x=\"hour_of_week\",y=\"avg_delay\",sort=\"hour_of_week\")\n",
    "\n",
    "    \n",
    "info = \n",
    "    SELECT ts\n",
    "        , carrier\n",
    "        , B.name AS carrier_name\n",
    "        , origin\n",
    "        , C.name AS origin_name      \n",
    "        , C.country AS origin_country      \n",
    "        , C.lat AS origin_lat       \n",
    "        , C.lng AS origin_lng    \n",
    "        , dest\n",
    "        , D.name AS dest_name\n",
    "        , D.country AS dest_country    \n",
    "        , D.lat AS dest_lat       \n",
    "        , D.lng AS dest_lng    \n",
    "        , delay\n",
    "    FROM flights AS A\n",
    "    LEFT OUTER JOIN airlines AS B\n",
    "        ON A.carrier = B.iata\n",
    "    LEFT OUTER JOIN airports AS C\n",
    "        ON A.origin = C.iata\n",
    "    LEFT OUTER JOIN airports AS D\n",
    "        ON A.dest = D.iata\n",
    "    WHERE C.lat IS NOT NULL AND C.lng IS NOT NULL\n",
    "        AND D.lat IS NOT NULL AND D.lng IS NOT NULL\n",
    "\n",
    "info_us = \n",
    "    SELECT * WHERE origin_country = dest_country AND origin_country = 'United States'\n",
    "    PERSIST\n",
    "    \n",
    "SELECT origin, AVG(delay) AS delay FROM info_us GROUP BY origin\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"origin\",y=\"delay\",sort=\"delay\", title=\"By Origin\")\n",
    "\n",
    "SELECT dest, AVG(delay) AS delay FROM info_us GROUP BY dest\n",
    "SELECT * ORDER BY delay DESC LIMIT 10\n",
    "OUTPUT USING plot_bar(x=\"dest\",y=\"delay\",sort=\"delay\", title=\"By Dest\")\n",
    "\n",
    "top = \n",
    "    SELECT carrier, COUNT(*) AS ct \n",
    "    FROM info_us GROUP BY carrier \n",
    "    ORDER BY ct DESC LIMIT {{n}}\n",
    "    YIELD DATAFRAME\n",
    "    \n",
    "info_top = \n",
    "    SELECT info_us.* FROM info_us INNER JOIN top ON info_us.carrier = top.carrier\n",
    "\n",
    "SELECT carrier_name, AVG(delay) AS delay FROM info_top GROUP BY carrier_name\n",
    "SELECT * ORDER BY delay DESC LIMIT {{n}}\n",
    "OUTPUT USING plot_bar(x=\"carrier_name\",y=\"delay\",sort=\"delay\", title=\"By Top Carriers\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de899e42",
   "metadata": {},
   "source": [
    "see we separated the pipeline to 3 parts\n",
    "\n",
    "the first to get reference data, second one to process and clean up the flights data\n",
    "\n",
    "the third one is to do the data analysis\n",
    "\n",
    "And we use YIELD to output result for the first and second modules\n",
    "\n",
    "Notice in the third one, we have this n in a jinja template\n",
    "\n",
    "this is how we parameterize the modules\n",
    "\n",
    "-\n",
    "\n",
    "now let's see how we chain them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-funds",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:20:06.434263Z",
     "start_time": "2021-07-16T06:19:49.374443Z"
    }
   },
   "outputs": [],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "metadata = fsql(metadata_query).run()\n",
    "flights = fsql(flights_query).run(\"spark\")\n",
    "fsql(eda_query, metadata, flights, n=30).run(\"spark\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0b4b0",
   "metadata": {},
   "source": [
    "we use fsql utility function to execute the sql strings\n",
    "\n",
    "for the first query, we run it without parameters, meaning that it will run using our native execution engine\n",
    "\n",
    "for the second and third, we run them on spark\n",
    "\n",
    "So if you modulize in this way, we can run each module with a different execution engine\n",
    "\n",
    "for example we can let the first step run on gpu using blazing, let the second run on dask, and let the third run on spark.\n",
    "\n",
    "This is normally unnecessary, but a more practical case is you can run with different size of spark clusters.\n",
    "\n",
    "-\n",
    "\n",
    "In the end I want to talk about testing\n",
    "\n",
    "For this particular case, it was not setup for the best testability but we still can use it as an example\n",
    "\n",
    "look at eda query, it yields the top dataframe\n",
    "\n",
    "so here is how we test with mock input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da862b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_airports = # a mock pandas dataframe\n",
    "mock_airlines = # a mock pandas dataframe\n",
    "mock_flights = # a mock pandas dataframe\n",
    "\n",
    "result = fsql(\n",
    "    eda_query, \n",
    "    airports=mock_airports,\n",
    "    airlines=mock_airlines,\n",
    "    flights=mock_mockflights,\n",
    "    n=10).run()\n",
    "\n",
    "df_to_assert_on = result[\"top\"].as_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a07ac2",
   "metadata": {},
   "source": [
    "instead of getting outputs from the first 2 modules, we directly provide mock pandas dataframes into the third module\n",
    "\n",
    "and we run it using native execution engine\n",
    "\n",
    "and assert on the yielded top dataframe\n",
    "\n",
    "everything will happen without Spark, it is just a normal test on pandas\n",
    "\n",
    "if you want to test on local spark, you only need to specify it in run, all other things will remain the same\n",
    "\n",
    "you just need to make sure your test environment can get a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea55ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
